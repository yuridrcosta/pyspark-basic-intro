# pyspark-basic-intro

De acordo com a [documentação oficial](https://spark.apache.org/docs/latest/), "Apache Spark é um motor (*engine*) de análise unificada para o processamento em larga escala de dados", em tradução livre. Trata-se de um sistema de processamento distribuído utilizado para Big Data que tem sido mias rápido que os sistemas anteriores como o (Apache Hadoop)[https://hadoop.apache.org/].

Esse repositório apresenta (até o momento) uma introdução rápida ao **Spark SQL**, que é o módulo para trabalhar com dados estruturados. É um objetivo produzir uma introdução à outros módulos módulos, como **Spark Streaming** (que realiza o processamento em tempo real de fluxos de dados) e **MLlib** (que fornece vários algoritmos tradicionais de aprendizagem de máquina).

# Conteúdo relevante

 - [PySpark Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/index.html)
 - [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/reference/index.html)
